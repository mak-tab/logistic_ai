from unsloth import FastLanguageModel
import torch

# 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–≤–æ—é –¢–û–õ–¨–ö–û –ß–¢–û –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "lora_model", # –ü–∞–ø–∫–∞, –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏–ª—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç (–ø—Ä–æ–≤–µ—Ä—å –∏–º—è –ø–∞–ø–∫–∏ –≤ training.py)
    max_seq_length = 2048,
    dtype = None,
    load_in_4bit = True,
)
FastLanguageModel.for_inference(model)

# 2. –¢–µ—Å—Ç–æ–≤—ã–π "–≥—Ä—è–∑–Ω—ã–π" –ø–æ—Å—Ç (–∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–µ –±—ã–ª–æ –≤ –æ–±—É—á–µ–Ω–∏–∏!)
test_post = """
üî•üî•üî• –°–†–û–ß–ù–û!
–¢–∞—à–∫–µ–Ω—Ç - –ë—É—Ö–∞—Ä–∞
–ù—É–∂–µ–Ω —Ä–µ—Ñ, 20 —Ç–æ–Ω–Ω.
–ì—Ä—É–∑: –º–æ—Ä–æ–∂–µ–Ω–æ–µ.
–û–ø–ª–∞—Ç–∞ 3.000.000 —Å—É–º –Ω–∞–ª.
–ó–≤–æ–Ω–∏—Ç—å: +998 90 123 45 67
"""

# 3. –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç (–¢–û–ß–ù–û –¢–ê–ö–û–ô –ñ–ï, –∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)
prompt = f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
You are a logistics AI. Extract shipments from the text into a JSON list.

### Input:
{test_post}

### Response:
"""

# 4. –ó–∞–ø—É—Å–∫–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
inputs = tokenizer([prompt], return_tensors = "pt").to("cuda")

outputs = model.generate(
    **inputs, 
    max_new_tokens = 512, 
    use_cache = True
)

# 5. –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
result = tokenizer.batch_decode(outputs)
print("\n=== –†–ï–ó–£–õ–¨–¢–ê–¢ ===\n")
print(result[0].split("### Response:")[-1].replace("<|endoftext|>", ""))